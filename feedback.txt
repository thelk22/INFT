You have not put your name below the title. Your title describes your work, as does your abstract, but it would be better if your abstract included a brief summary of your main findings. You have a clear and concise introduction to your paper. Some more general introduction to BSE and the different trading algorithms could have been included. Your experimental design is well motivated - I like the discussion of adaptability and convergence to motivate the use of dynamic markets, and it is nice to see that you make use of AWS SageMaker to run experiments. You set up a hypothesis which is good to see, but I would frame the alternative as mu != 0, rather than mu>0, as it could be that profit decreases when you alter k. You repeat a paragraph staring "It is likely that adopters of the PRSH algorithm...". It's good that you introduce market shocks, and it's nice that you plotted a graph of trades generated under a shock, although you should give brief details on what traders etc. generated this data (in the figure caption). It's good to see that you give a detailed description of your experiment design, justifying all your decisions. You then perform some exploratory data analysis, but there are no details on what you did here (how many runs? Is this the configuration you detailed before?) - be specific. In figs 2 and 3 you present exploratory data showing average profits, but such values are only properly informative if you also show some indication of variance (e.g. stddev or stderr) or uncertainty (e.g. confidence intervals, CIs) around the mean value. If you had plotted these with confidence intervals, or as a set of box-plots side-by-side, you should be able to eyeball whether the profit of PRSH is significantly higher than ZIP, and also whether or not profit really does increase with k. Again, in Fig 4, rather than bars to represent mean, a box-plot would be more informative. You then perform detailed statistical analysis; you consider if the data is normally distributed and you use a mon-parametric test. This is good. However, rather than run a sequence of pairwise A/B t-tests to data that is correctly characterised as A/B/C/D/E (because you are comparing 7 different k values) is not good practice: you should instead have used an ANOVA-style test (if data is normal) or a corresponding nonparametric test such as Kruskall-Wallis. This would have shown with one test that varying k results in no significant difference. You then introduce a set of alternative mutation functions - I like how you describe your rationale for these, and explain why the default mutation may be problematic. Good. You use an offset function to test these - it would be mice if you had shown the function (ie plot the spike) to complement your description. Fig 9 (comparison of profits of mutation functions) would be better a box plots (see comments before). You then use a sign-ranked test to show M2 and M4 outperform M1 - I particularly like that you then spend time interpreting this result and explaining why M2 and M4 perform better. Your conclusion says "the choice of k impacts performance" - I don't think you showed this, there was no significant difference. You end by suggesting an extension, but you do not have time to implement. Overall, this is a good attempt.

